# bot.py

import logging
import os
import requests
import pandas as pd
from bs4 import BeautifulSoup
from telegram import Update
from telegram.ext import (
    Application,
    CommandHandler,
    MessageHandler,
    filters,
    ConversationHandler,
    ContextTypes
)
from config import TELEGRAM_TOKEN, HEADERS, DEFAULT_AREA

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# –°–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è ConversationHandler
PAGES, QUERY = range(2)

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> int:
    """–ù–∞—á–∞–ª–æ –¥–∏–∞–ª–æ–≥–∞ –∏ –∑–∞–ø—Ä–æ—Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–∞–Ω–∏—Ü"""
    await update.message.reply_text(
        "üëã –ü—Ä–∏–≤–µ—Ç! –Ø –ø–æ–º–æ–≥—É –Ω–∞–π—Ç–∏ –≤–∞–∫–∞–Ω—Å–∏–∏ –Ω–∞ HH.ru\n\n"
        "–°–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü –ø–∞—Ä—Å–∏—Ç—å? (–≤–≤–µ–¥–∏—Ç–µ —á–∏—Å–ª–æ)"
    )
    return PAGES

async def pages(update: Update, context: ContextTypes.DEFAULT_TYPE) -> int:
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–∞–Ω–∏—Ü –∏ –∑–∞–ø—Ä–æ—Å –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
    try:
        pages = int(update.message.text)
        if pages < 1 or pages > 50:
            raise ValueError
        context.user_data['pages'] = pages
        await update.message.reply_text(
            f"‚úÖ –û—Ç–ª–∏—á–Ω–æ! –ü–∞—Ä—Å–∏—Ç—å {pages} —Å—Ç—Ä–∞–Ω–∏—Ü(—ã)\n\n"
            "–í–≤–µ–¥–∏—Ç–µ –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å (–Ω–∞–ø—Ä–∏–º–µ—Ä: 'python —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫')"
        )
        return QUERY
    except (ValueError, TypeError):
        await update.message.reply_text(
            "‚ùå –û—à–∏–±–∫–∞! –í–≤–µ–¥–∏—Ç–µ —á–∏—Å–ª–æ –æ—Ç 1 –¥–æ 50\n"
            "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞:"
        )
        return PAGES

async def query(update: Update, context: ContextTypes.DEFAULT_TYPE) -> int:
    """–ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    query_text = update.message.text.strip()
    pages = context.user_data['pages']

    await update.message.reply_text(
        f"üîç –ù–∞—á–∏–Ω–∞—é –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É '{query_text}'...\n"
        f"‚è≥ –ü–∞—Ä—Å–∏–Ω–≥ {pages} —Å—Ç—Ä–∞–Ω–∏—Ü(—ã). –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 2-5 –º–∏–Ω—É—Ç..."
    )

    try:
        urls = parse_vacancy_links(pages, query_text)

        if not urls:
            await update.message.reply_text(
                "‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å."
            )
            return ConversationHandler.END

        await update.message.reply_text(
            f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(urls)} –≤–∞–∫–∞–Ω—Å–∏–π. –°–æ–±–∏—Ä–∞—é –¥–µ—Ç–∞–ª–∏..."
        )

        data = collect_vacancy_data(urls)

        if not data:
            await update.message.reply_text(
                "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–æ –≤–∞–∫–∞–Ω—Å–∏—è–º."
            )
            return ConversationHandler.END

        csv_path, excel_path = save_data(data)

        with open(csv_path, 'rb') as csv:
            await update.message.reply_document(
                document=csv,
                filename="vacancies.csv",
                caption="üìÑ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ CSV"
            )

        with open(excel_path, 'rb') as excel:
            await update.message.reply_document(
                document=excel,
                filename="vacancies.xlsx",
                caption="üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ Excel"
            )

        os.remove(csv_path)
        os.remove(excel_path)

        await update.message.reply_text(
            "‚úÖ –ì–æ—Ç–æ–≤–æ! –í—ã –º–æ–∂–µ—Ç–µ –æ—Ç–∫—Ä—ã—Ç—å —Ñ–∞–π–ª—ã –≤ Excel –∏–ª–∏ —Ç–∞–±–ª–∏—Ü–∞—Ö.\n"
            "–•–æ—Ç–∏—Ç–µ –Ω–∞–π—Ç–∏ —á—Ç–æ-—Ç–æ –µ—â—ë? –ù–∞–∂–º–∏—Ç–µ /start"
        )

    except Exception as e:
        logger.exception(e)
        await update.message.reply_text(
            f"‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}\n"
            "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ –∏–ª–∏ –∏–∑–º–µ–Ω–∏—Ç–µ –∑–∞–ø—Ä–æ—Å."
        )

    return ConversationHandler.END

def parse_vacancy_links(pages: int, query: str) -> list:
    """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Å—ã–ª–æ–∫ –Ω–∞ –≤–∞–∫–∞–Ω—Å–∏–∏"""
    urls = []

    for page in range(pages):
        url = (
            f"https://hh.ru/search/vacancy?"
            f"text={query}&"
            f"area={DEFAULT_AREA}&"
            f"page={page}"
        )

        try:
            response = requests.get(url, headers=HEADERS)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            for link in soup.find_all('a', {'data-qa': 'serp-item__title'}):
                vacancy_url = link['href']
                if 'hh.ru/vacancy' in vacancy_url and vacancy_url not in urls:
                    urls.append(vacancy_url)
        except Exception as e:
            logger.error(f"Error parsing page {page}: {str(e)}")

    return urls

def collect_vacancy_data(urls: list) -> list:
    """–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ –∫–∞–∂–¥–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏"""
    data = []

    for url in urls:
        try:
            response = requests.get(url, headers=HEADERS)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            title = soup.find('h1', {'data-qa': 'vacancy-title'})
            title = title.get_text(strip=True) if title else "–ù–µ —É–∫–∞–∑–∞–Ω–æ"

            company = soup.find('a', {'data-qa': 'vacancy-company-name'})
            company = company.get_text(strip=True) if company else "–ù–µ —É–∫–∞–∑–∞–Ω–∞"

            address = soup.find('p', {'data-qa': 'vacancy-view-location'})
            if not address:
                address = soup.find('span', {'data-qa': 'vacancy-view-raw-address'})
            address = address.get_text(strip=True) if address else "–ù–µ —É–∫–∞–∑–∞–Ω"

            salary = soup.find('span', {'data-qa': 'vacancy-salary-compensation'})
            salary = salary.get_text(strip=True) if salary else "–ù–µ —É–∫–∞–∑–∞–Ω–∞"

            tax = "–Ω–∞ —Ä—É–∫–∏" if "–Ω–∞ —Ä—É–∫–∏" in salary.lower() else "–¥–æ –≤—ã—á–µ—Ç–∞ –Ω–∞–ª–æ–≥–æ–≤"

            data.append({
                "–ù–∞–∑–≤–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏": title,
                "–ö–æ–º–ø–∞–Ω–∏—è": company,
                "–ê–¥—Ä–µ—Å": address,
                "–ó–∞—Ä–ø–ª–∞—Ç–∞": salary,
                "–í—ã–¥–∞—á–∞": tax,
                "–°—Å—ã–ª–∫–∞": url
            })

        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {url}: {str(e)}")

    return data

def save_data(data: list) -> tuple:
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–∞–π–ª—ã"""
    df = pd.DataFrame(data)

    csv_path = "vacancies.csv"
    excel_path = "vacancies.xlsx"

    df.to_csv(csv_path, index=False, encoding='utf-8-sig')
    df.to_excel(excel_path, index=False)

    return csv_path, excel_path

async def cancel(update: Update, context: ContextTypes.DEFAULT_TYPE) -> int:
    """–û—Ç–º–µ–Ω–∞ –æ–ø–µ—Ä–∞—Ü–∏–∏"""
    await update.message.reply_text(
        "‚ùå –û–ø–µ—Ä–∞—Ü–∏—è –æ—Ç–º–µ–Ω–µ–Ω–∞. –ß—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å –∑–∞–Ω–æ–≤–æ, –Ω–∞–∂–º–∏—Ç–µ /start"
    )
    return ConversationHandler.END

def main() -> None:
    """–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞"""
    application = Application.builder().token(TELEGRAM_TOKEN).build()

    conv_handler = ConversationHandler(
        entry_points=[CommandHandler('start', start)],
        states={
            PAGES: [MessageHandler(filters.TEXT & ~filters.COMMAND, pages)],
            QUERY: [MessageHandler(filters.TEXT & ~filters.COMMAND, query)],
        },
        fallbacks=[CommandHandler('cancel', cancel)],
    )

    application.add_handler(conv_handler)
    application.add_handler(CommandHandler("cancel", cancel))

    logger.info("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω")
    application.run_polling()
    logger.info("–ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

if __name__ == '__main__':
    main()
